{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJFO6MsRD9Ix1aTqzVh/eX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmed2045/sentiment-analysis-nlp-/blob/main/NLP_Sentiment_analysisipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Wxo8Rq8Ii0Z",
        "outputId": "395af41e-2407-4f31-e6c8-456325be012d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "#unzip the file data\n",
        "from zipfile import ZipFile\n",
        "file_name = 'data.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#will be used to get review\n",
        "import os\n",
        "import glob\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print (os.listdir(\"data/\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz-e1RJTIjun",
        "outputId": "49f7d1ec-7b2d-4741-d276-7f2dd6f20731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test', 'train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use as temporary variables to store data\n",
        "data = []\n",
        "data2= []\n",
        "data3 = []\n",
        "data4=[]\n",
        "\n",
        "#use as temporary variables to be used in appending\n",
        "\n",
        "temp= []\n",
        "temp2= []\n",
        "temp3= []\n",
        "temp4=[]\n",
        "\n",
        "#to store train and test labels\n",
        "labels_train = []\n",
        "labels_test = []\n",
        "\n",
        "#to store train and test data\n",
        "data_test =[]\n",
        "data_train =[]\n",
        "\n",
        "#get the positive train data\n",
        "for path in glob.glob(\"data/train/positive/*\"):\n",
        "  label = path.split(\"/\")[-1]\n",
        "  infile = open(path,\"r\")\n",
        "  contents = infile.read()\n",
        "  #read file\n",
        "  soup = BeautifulSoup(contents)\n",
        "  #find all paragraphs under review_text tag\n",
        "  data = soup.find_all('review_text')\n",
        "  temp.extend(data)\n",
        "\n",
        "\n",
        "#get the negative train data\n",
        "\n",
        "\n",
        "for path in glob.glob(\"data/train/negative/*\"):\n",
        "  label = path.split(\"/\")[-1]\n",
        "  print(label)\n",
        "  infile = open(path,\"r\")\n",
        "  contents = infile.read()\n",
        "  soup = BeautifulSoup(contents)\n",
        "  data2 = soup.find_all('review_text')\n",
        "  temp2.extend(data2)\n",
        "\n",
        "\n",
        "#append positive train labels and data\n",
        "for sentence in temp:\n",
        "  data_train.append(sentence.get_text())\n",
        "  labels_train.append('positive')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#append negative train labels and data\n",
        "for sentence in temp2:\n",
        "  data_train.append(sentence.get_text())\n",
        "  labels_train.append('negative')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#get the positive test data\n",
        "\n",
        "\n",
        "for path in glob.glob(\"data/test/positive/*\"):\n",
        "  label = path.split(\"/\")[-1]\n",
        "  infile = open(path,\"r\")\n",
        "  contents = infile.read()\n",
        "  soup = BeautifulSoup(contents)\n",
        "  data3= soup.find_all('review_text')\n",
        "  temp3.extend(data3)\n",
        "\n",
        "#get the positive test data\n",
        "\n",
        "for path in glob.glob(\"data/test/negative/*\"):\n",
        "  label = path.split(\"/\")[-1]\n",
        "  print(label)\n",
        "  infile = open(path,\"r\")\n",
        "  contents = infile.read()\n",
        "  soup = BeautifulSoup(contents)\n",
        "  data4 = soup.find_all('review_text')\n",
        "  temp4.extend(data4)\n",
        "\n",
        "\n",
        "#append positive test labels  and data\n",
        "\n",
        "for sentence in temp3:\n",
        "  data_test.append(sentence.get_text())\n",
        "  labels_test.append('positive')\n",
        "\n",
        "\n",
        "#append negative test labels  and data\n",
        "\n",
        "for sentence in temp4:\n",
        "  data_test.append(sentence.get_text())\n",
        "  labels_test.append('negative')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBI9-YdZIjwr",
        "outputId": "d8a3bae6-2415-464d-eed4-b1bb854e70ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "electronics_negative.review\n",
            "dvd_negative.review\n",
            "books_negative.review\n",
            "negative.review\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing data\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopword = stopwords.words('english')\n",
        "count_vect = CountVectorizer(stop_words = stopword)\n",
        "X = count_vect.fit_transform(data_train)\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_tfidf = tfidf_transformer.fit_transform(X)\n",
        "X_tfidf.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZFKfTqQIv5r",
        "outputId": "248b9465-ebd5-4887-f364-f8587b147e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 36410)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.linear_model import *\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.tree import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,ConfusionMatrixDisplay,roc_curve,auc,RocCurveDisplay,classification_report\n"
      ],
      "metadata": {
        "id": "p6k2zhKrJYQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use Support vector machine model in our pretrained model\n",
        "vectorizer=TfidfVectorizer(analyzer='char_wb',ngram_range=(3,5),min_df=0.01,max_df=0.5)\n",
        "clf=SVC(kernel='rbf')\n",
        "pipe=make_pipeline(vectorizer,clf)\n"
      ],
      "metadata": {
        "id": "WrYxK79GIv_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the model with train data and label\n",
        "pipe.fit(data_train,labels_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FweO1InyI6Fq",
        "outputId": "5bccf134-4cf0-4070-a801-21d75eb0d15a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidfvectorizer',\n",
              "                 TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=0.01,\n",
              "                                 ngram_range=(3, 5))),\n",
              "                ('svc', SVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict using the test data\n",
        "y_pred=pipe.predict(data_test)\n",
        "#calculate accuracy\n",
        "accuracy=accuracy_score(labels_test,y_pred)*100\n",
        "\n",
        "print(\"Accuracy is : \",accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz7BjrmwI6Mw",
        "outputId": "9d113637-603f-48f0-d641-a172a8725eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is :  82.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#try our model with different sentences\n",
        "\n",
        "input=['you are good']\n",
        "\n",
        "y_pred=pipe.predict(input)\n",
        "\n",
        "print(\"your sentence is : \",y_pred[0])\n",
        "\n",
        "\n",
        "input=['you are terrible ']\n",
        "\n",
        "yd=pipe.predict(input)\n",
        "\n",
        "print(\"your sentence is : \",yd[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sQDzKegI6Sc",
        "outputId": "de95b425-c713-4f26-9113-e7f2fcd8c747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your sentence is :  positive\n",
            "your sentence is :  negative\n"
          ]
        }
      ]
    }
  ]
}